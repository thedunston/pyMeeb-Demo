{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "636e9d58-6945-4906-8dc3-6ab911ce64c6",
   "metadata": {},
   "source": [
    "## Threat Hunting Report with Logarithms\n",
    "### Logarithms\n",
    "You will need to tune the threshold when using a logarithm, though the threshold of -3 is set, by default. The logarithm is used because it can compress large datasets and help rare events to stand out. The term \"compress\" in this context means to group common data points. A value of -3 provides a good balance of whether or not an anomaly will stand out.\n",
    "\n",
    "Changing the values will help you identify anomalies.\n",
    "\n",
    "### Heterogenous Dataset\n",
    "If the dataset is heterogenous, then you'll likely need a higher threshold (thus less sensitivity) to test it and start at around -2 or -2.5. That is because there is more variablity in the data so less sensitivity will lead to anomalies standing out easier.\n",
    "\n",
    "The context is if you are analzying processes, for example, across different operating system versions or roles of the OS are different (eg. IIS web servers versus Exchange server). Not recommended, however.\n",
    "\n",
    "### Homogenous Dataset\n",
    "If you have a similar dataset, then you'd want a lower threshold so the -3 is a good start.\n",
    "\n",
    "It is able to find 2 anomalies. The first value is the number of hosts the file is on and then the logarithmic value. Since the dataset is homogenous, the lower threshold shows rare processes running.\n",
    "\n",
    "### Dataset size\n",
    "Larger datasets provide higher confidence in detecting anomalies so a lower threshold works well (-3, -3.5, -4)\n",
    "\n",
    "Smaller datasets have less confidence so a higher threshold is necessary -2 or -1 (for small datasets).\n",
    "\n",
    "### Sample Analysis\n",
    "\n",
    "```\n",
    "[snipped for brevity]\n",
    "[200 -1.838629 C:\\ProgramData\\Ashampoo Winzip AshampooWinZip.exe]\n",
    "[200 -1.838629 C:\\Program Files\\WebServer_11.24.0.0_x64__8wekyb3d8bbwe\\WebServer.exe]\n",
    "[200 -1.838629 C:\\Program Files\\SessionManager_11.24.0.0_x64__8wekyb3d8bbwe\\SessionManager.exe]\n",
    "[200 -1.838629 C:\\ProgramData\\Dell\\Supportassist\\DellSupportAssist.exe]\n",
    "[200 -1.838629 C:\\Program Files (x86)\\Twitch Twitch.exe]\n",
    "[200 -1.838629 C:\\Program Files (x86)\\Microsoft Edge\\Application MicrosoftEdge.exe]\n",
    "[200 -1.838629 C:\\Windows\\SystemHealth.exe]\n",
    "[200 -1.838629 C:\\Windows\\system32\\taskmgr.exe]\n",
    "[199 -1.840806 C:\\Program Files (x86)\\iPod\\ iTunes AppleiTunes.exe]\n",
    "[193 -1.854101 C:\\Windows\\taskmgr.exe]\n",
    "[2 -3.838629 C:\\ProgramData\\system32\\csrss.exe]\n",
    "[1 -4.139659 C:\\Program Files (x86)\\iPod\\ iTunes AppleTunes.exe]\n",
    "```\n",
    "\n",
    "Note how there is a C:\\Windows\\taskmgr.exe on 193 hosts, but didn't show up as an anomaly with a threshold of -3. That is why you need to change the threshold so that you better spot anomalies that may exist outside of a given threshold. While larger datasets have higher confidence of anomalies with a lower threshold, some anomalies could still be missed. Accordingly, all of these factors are the reason you need to change the the threshold during your analysis. Also, the legit taskmgr.exe file is located in C:\\Windows\\system32\\taskmgr.exe. If this was a real system, this would likely be a mass compromise UNLESS there is a custom program in that path or a third-party program had a similar name process. CONTEXT! CONTEXT! CONTEXT!\n",
    "\n",
    "In the above output, you can also see the difference with the 'AppleiTunes.exe' path and the 'AppleTunes.exe' path.\n",
    "\n",
    "GitHub repo contains a Go based version of the logarithmic functions below and another tool to create a baseline and then compare the remaining systems against the baseline. https://github.com/thedunston/goMeeb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d912631-5e88-4fdc-9e1d-ea1e368a9d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import math\n",
    "import threading\n",
    "import queue\n",
    "import pandas as pd\n",
    "import jinja2\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Retrieve list of CSV files from the specified directory.\n",
    "def get_csv_files(directory):\n",
    "    # Recursively gather all CSV files.\n",
    "    files = [str(path) for path in Path(directory).rglob('*.csv')]\n",
    "    if not files:\n",
    "\n",
    "        raise FileNotFoundError(f\"No CSV files found in directory {directory}\")\n",
    "    \n",
    "    return files\n",
    "\n",
    "# Determine the number of threads to use based on the number of files.\n",
    "def determine_num_threads(files):\n",
    "    \n",
    "    # Basic method to determine number of threads.\n",
    "    # This is helpful for dozens of CSV files..\n",
    "    num_threads = len(files) // 2\n",
    "    if num_threads > 10:\n",
    "        num_threads = 10\n",
    "    elif num_threads < 1:\n",
    "        num_threads = 1\n",
    "    return num_threads\n",
    "\n",
    "# Process each CSV file and return the count of occurrences for the specified header.\n",
    "def process_file(file, header, data_queue):\n",
    "    data = defaultdict(int)\n",
    "    total_entries = 0\n",
    "\n",
    "    # Open the CSV file.\n",
    "    with open(file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        # Check if the header is present in the CSV file.\n",
    "        if header not in reader.fieldnames:\n",
    "            print(f\"Header {header} not found in file {file}\")\n",
    "            return\n",
    "        \n",
    "        # Count occurrences of each value under the specified header.\n",
    "        for row in reader:\n",
    "            value = row[header]\n",
    "            data[value] += 1\n",
    "            total_entries += 1\n",
    "    \n",
    "    # Put the processed data into the queue.\n",
    "    data_queue.put((data, total_entries))\n",
    "\n",
    "# Aggregate data from all files for the specified header.\n",
    "def aggregate_data(files, header, num_threads):\n",
    "    data_queue = queue.Queue()\n",
    "    threads = []\n",
    "\n",
    "    # Create and start threads for concurrent processing.\n",
    "    for i in range(num_threads):\n",
    "        t = threading.Thread(target=process_files_thread, args=(files[i::num_threads], header, data_queue))\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "\n",
    "    # Wait for all threads to finish.\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    aggregated_data = defaultdict(int)\n",
    "    total_entries = 0\n",
    "\n",
    "    # Collect data from the queue and aggregate it.\n",
    "    while not data_queue.empty():\n",
    "        data, count = data_queue.get()\n",
    "        for key, value in data.items():\n",
    "            aggregated_data[key] += value\n",
    "        total_entries += count\n",
    "\n",
    "    return aggregated_data, total_entries\n",
    "\n",
    "# Process files.\n",
    "def process_files_thread(files, header, data_queue):\n",
    "    for file in files:\n",
    "        process_file(file, header, data_queue)\n",
    "\n",
    "# Identify anomalies based on the specified threshold.\n",
    "# ChatGPT help with this...math. :)\n",
    "def identify_anomalies(data, total_entries, threshold, filter_terms=None):\n",
    "    anomalies = []\n",
    "    # Add this new filtering logic (keep all existing comments)\n",
    "    if filter_terms:\n",
    "        filter_terms = [term.strip().lower() for term in filter_terms if term.strip()]\n",
    "    \n",
    "    for value, count in data.items():\n",
    "        # Add this new filter check (keep all other existing code)\n",
    "        if filter_terms and any(term in value.lower() for term in filter_terms):\n",
    "            continue\n",
    "            \n",
    "        proportion = float(count) / float(total_entries)\n",
    "        log_proportion = math.log10(proportion)\n",
    "        if log_proportion < threshold:\n",
    "            anomalies.append([count, log_proportion, value])\n",
    "    anomalies.sort(key=lambda x: x[1])\n",
    "    return anomalies\n",
    "    \n",
    "def print_results_dataframe(results):\n",
    "    import pandas as pd\n",
    "    import sys\n",
    "    \n",
    "    # Create and clean the DataFrame\n",
    "    df = pd.DataFrame(results, columns=['Count', 'Log Proportion', 'Value'])\n",
    "    \n",
    "    # 1. Remove all problematic whitespace\n",
    "    df['Value'] = df['Value'].str.strip()\n",
    "    df['Value'] = df['Value'].str.replace(r'\\s+', ' ', regex=True)\n",
    "    \n",
    "    # 2. Format numbers consistently\n",
    "    df['Log Proportion'] = df['Log Proportion'].apply(lambda x: f\"{float(x):.5f}\")\n",
    "    df['Count'] = df['Count'].astype(str)\n",
    "    \n",
    "    # 3. Environment detection and printing\n",
    "    if 'ipykernel' in sys.modules:  # Jupyter Notebook/Lab\n",
    "        from IPython.display import HTML\n",
    "        html = df.to_html(\n",
    "            index=False,\n",
    "            justify='left',\n",
    "            border=0,\n",
    "            classes=['left-aligned-table'],\n",
    "            na_rep=''\n",
    "        )\n",
    "        # CSS injection for perfect left alignment\n",
    "        display(HTML(f\"\"\"\n",
    "        <style>\n",
    "        .left-aligned-table td, .left-aligned-table th {{\n",
    "            text-align: left !important;\n",
    "            font-family: monospace;\n",
    "            white-space: pre;\n",
    "        }}\n",
    "        </style>\n",
    "        {html}\n",
    "        \"\"\"))\n",
    "    else:  # Terminal or script\n",
    "        from tabulate import tabulate\n",
    "        print(tabulate(\n",
    "            df,\n",
    "            headers='keys',\n",
    "            tablefmt='psql',\n",
    "            showindex=False,\n",
    "            stralign='left',\n",
    "            numalign='left'\n",
    "        ))\n",
    "\n",
    "try:\n",
    "\n",
    "    ################ BEGIN EDITING HERE #########################\n",
    "\n",
    "    # Directory path where the CSV files are stored.\n",
    "    # Download the threat hunting dataset: https://github.com/mosse-security/threat-hunting-samples and update the path.\n",
    "    directory = \"dataset-1/w32processes\"\n",
    "    \n",
    "    # Header in the CSV files to filter on. Update this as needed based on the CSV file you use.\n",
    "    header = \"\"\n",
    "    \n",
    "    # Comma-separated values to exclude from results\n",
    "    filter_str = \"\"\n",
    "    \n",
    "    # Threshold for identifying anomalies. This value will need to be adjusted \n",
    "    # based on the size of the dataset and the variability in the data.\n",
    "    # Careful here because rendering the results in the browser can\n",
    "    # cause it to crash if there are a lot of results.\n",
    "    threshold = -2.0\n",
    "\n",
    "    ################ END EDITING HERE #########################\n",
    "    files = get_csv_files(directory)\n",
    "    num_threads = determine_num_threads(files)\n",
    "    aggregated_data, total_entries = aggregate_data(files, header, num_threads)\n",
    "    \n",
    "    # Parse filter string and pass to identify_anomalies\n",
    "    filter_terms = [term.strip() for term in filter_str.split(',')] if filter_str else None\n",
    "    \n",
    "    # Add filter_terms parameter to this call (keep all other params the same)\n",
    "    results = identify_anomalies(aggregated_data, total_entries, threshold, filter_terms)\n",
    "    print_results_dataframe(results)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9138941-8715-45b0-9c5e-0b4efb2d7d70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
